{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_indexes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-8331686960a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcamembert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_roberta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcamembert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_task_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_vocab_size_and_dictionary_per_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_batch_size_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcamembert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_iterators_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_processing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_bpe_token_to_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_indexes'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0,\"../../../../..\")\n",
    "from camembert.downstream.finetune.env.dir.data_dir import get_dir_data\n",
    "from camembert.downstream.finetune.model.architecture.get_model import get_model_multi_task_bert\n",
    "from camembert.downstream.finetune.model.settings import TASKS_PARAMETER\n",
    "from camembert.downstream.finetune.env.dir.pretrained_model_dir import DIR_2_STAT_MAPPING\n",
    "from camembert.downstream.finetune.env.dir.pretrained_model_dir import BERT_MODEL_DIC\n",
    "from camembert.downstream.finetune.transformers.transformers.tokenization_bert import BertTokenizer\n",
    "from camembert.tokenization_camembert import CamembertTokenizer\n",
    "from camembert.downstream.finetune.transformers.transformers.modeling_multitask import BertMultiTask\n",
    "from camembert.downstream.finetune.transformers.transformers.modeling_roberta import RobertaModel, RobertaConfig\n",
    "from camembert.downstream.finetune.trainer.tools.multi_task_tools import get_vocab_size_and_dictionary_per_task, update_batch_size_mean\n",
    "from camembert.downstream.finetune.io_.bert_iterators_tools.string_processing import get_indexes, from_bpe_token_to_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : no dictionaries and voc_sizes needed\n",
      "BertMultitask instantiated with RobertaModel encoder\n",
      "INFO : from loading from pretrained method (assuming loading original google model : need to rename some keys {'roberta': 'encoder', 'lm_head.decoder': 'head.mlm.predictions.decoder', 'lm_head.dense': 'head.mlm.predictions.transform.dense', 'lm_head.bias': 'head.mlm.predictions.bias', 'lm_head.layer_norm': 'head.mlm.predictions.transform.LayerNorm'})\n",
      "MODEL  replacing key roberta.embeddings.word_embeddings.weight from  loaded statedic  with encoder.embeddings.word_embeddings.weight \n",
      "MODEL  replacing key roberta.embeddings.position_embeddings.weight from  loaded statedic  with encoder.embeddings.position_embeddings.weight \n",
      "MODEL  replacing key roberta.embeddings.token_type_embeddings.weight from  loaded statedic  with encoder.embeddings.token_type_embeddings.weight \n",
      "MODEL  replacing key roberta.embeddings.LayerNorm.weight from  loaded statedic  with encoder.embeddings.LayerNorm.weight \n",
      "MODEL  replacing key roberta.embeddings.LayerNorm.bias from  loaded statedic  with encoder.embeddings.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.0.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.0.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.0.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.0.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.0.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.0.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.0.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.0.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.0.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.0.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.0.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.0.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.0.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.0.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.0.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.0.output.dense.weight from  loaded statedic  with encoder.encoder.layer.0.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.0.output.dense.bias from  loaded statedic  with encoder.encoder.layer.0.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.0.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.0.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.0.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.0.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.1.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.1.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.1.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.1.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.1.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.1.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.1.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.1.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.1.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.1.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.1.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.1.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.1.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.1.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.1.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.1.output.dense.weight from  loaded statedic  with encoder.encoder.layer.1.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.1.output.dense.bias from  loaded statedic  with encoder.encoder.layer.1.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.1.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.1.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.1.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.1.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.2.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.2.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.2.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.2.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.2.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.2.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.2.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.2.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.2.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.2.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.2.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.2.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.2.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.2.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.2.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.2.output.dense.weight from  loaded statedic  with encoder.encoder.layer.2.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.2.output.dense.bias from  loaded statedic  with encoder.encoder.layer.2.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.2.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.2.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.2.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.2.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.3.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.3.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.3.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.3.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.3.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.3.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.3.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.3.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.3.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.3.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.3.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.3.attention.self.value.bias \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL  replacing key roberta.encoder.layer.3.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.3.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.3.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.3.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.3.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.3.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.3.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.3.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.3.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.3.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.3.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.3.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.3.output.dense.weight from  loaded statedic  with encoder.encoder.layer.3.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.3.output.dense.bias from  loaded statedic  with encoder.encoder.layer.3.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.3.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.3.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.3.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.3.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.4.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.4.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.4.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.4.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.4.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.4.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.4.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.4.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.4.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.4.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.4.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.4.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.4.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.4.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.4.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.4.output.dense.weight from  loaded statedic  with encoder.encoder.layer.4.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.4.output.dense.bias from  loaded statedic  with encoder.encoder.layer.4.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.4.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.4.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.4.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.4.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.5.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.5.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.5.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.5.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.5.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.5.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.5.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.5.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.5.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.5.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.5.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.5.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.5.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.5.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.5.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.5.output.dense.weight from  loaded statedic  with encoder.encoder.layer.5.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.5.output.dense.bias from  loaded statedic  with encoder.encoder.layer.5.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.5.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.5.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.5.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.5.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.6.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.6.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.6.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.6.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.6.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.6.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.6.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.6.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.6.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.6.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.6.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.6.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.6.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.6.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.6.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.6.output.dense.weight from  loaded statedic  with encoder.encoder.layer.6.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.6.output.dense.bias from  loaded statedic  with encoder.encoder.layer.6.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.6.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.6.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.6.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.6.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.7.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.7.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.7.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.7.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.7.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.7.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.7.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.7.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.7.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.7.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.7.attention.output.LayerNorm.bias \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL  replacing key roberta.encoder.layer.7.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.7.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.7.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.7.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.7.output.dense.weight from  loaded statedic  with encoder.encoder.layer.7.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.7.output.dense.bias from  loaded statedic  with encoder.encoder.layer.7.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.7.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.7.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.7.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.7.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.8.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.8.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.8.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.8.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.8.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.8.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.8.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.8.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.8.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.8.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.8.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.8.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.8.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.8.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.8.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.8.output.dense.weight from  loaded statedic  with encoder.encoder.layer.8.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.8.output.dense.bias from  loaded statedic  with encoder.encoder.layer.8.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.8.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.8.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.8.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.8.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.9.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.9.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.9.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.9.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.9.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.9.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.9.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.9.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.9.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.9.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.9.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.9.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.9.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.9.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.9.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.9.output.dense.weight from  loaded statedic  with encoder.encoder.layer.9.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.9.output.dense.bias from  loaded statedic  with encoder.encoder.layer.9.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.9.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.9.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.9.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.9.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.10.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.10.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.10.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.10.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.10.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.10.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.10.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.10.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.10.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.10.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.10.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.10.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.10.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.10.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.10.intermediate.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.10.output.dense.weight from  loaded statedic  with encoder.encoder.layer.10.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.10.output.dense.bias from  loaded statedic  with encoder.encoder.layer.10.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.10.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.10.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.10.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.10.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.self.query.weight from  loaded statedic  with encoder.encoder.layer.11.attention.self.query.weight \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.self.query.bias from  loaded statedic  with encoder.encoder.layer.11.attention.self.query.bias \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.self.key.weight from  loaded statedic  with encoder.encoder.layer.11.attention.self.key.weight \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.self.key.bias from  loaded statedic  with encoder.encoder.layer.11.attention.self.key.bias \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.self.value.weight from  loaded statedic  with encoder.encoder.layer.11.attention.self.value.weight \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.self.value.bias from  loaded statedic  with encoder.encoder.layer.11.attention.self.value.bias \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.output.dense.weight from  loaded statedic  with encoder.encoder.layer.11.attention.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.output.dense.bias from  loaded statedic  with encoder.encoder.layer.11.attention.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.11.attention.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.11.attention.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.11.attention.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.encoder.layer.11.intermediate.dense.weight from  loaded statedic  with encoder.encoder.layer.11.intermediate.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.11.intermediate.dense.bias from  loaded statedic  with encoder.encoder.layer.11.intermediate.dense.bias \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL  replacing key roberta.encoder.layer.11.output.dense.weight from  loaded statedic  with encoder.encoder.layer.11.output.dense.weight \n",
      "MODEL  replacing key roberta.encoder.layer.11.output.dense.bias from  loaded statedic  with encoder.encoder.layer.11.output.dense.bias \n",
      "MODEL  replacing key roberta.encoder.layer.11.output.LayerNorm.weight from  loaded statedic  with encoder.encoder.layer.11.output.LayerNorm.weight \n",
      "MODEL  replacing key roberta.encoder.layer.11.output.LayerNorm.bias from  loaded statedic  with encoder.encoder.layer.11.output.LayerNorm.bias \n",
      "MODEL  replacing key roberta.pooler.dense.weight from  loaded statedic  with encoder.pooler.dense.weight \n",
      "MODEL  replacing key roberta.pooler.dense.bias from  loaded statedic  with encoder.pooler.dense.bias \n",
      "MODEL  replacing key lm_head.bias from  loaded statedic  with head.mlm.predictions.bias \n",
      "MODEL  replacing key lm_head.dense.weight from  loaded statedic  with head.mlm.predictions.transform.dense.weight \n",
      "MODEL  replacing key lm_head.dense.bias from  loaded statedic  with head.mlm.predictions.transform.dense.bias \n",
      "MODEL  replacing key lm_head.layer_norm.weight from  loaded statedic  with head.mlm.predictions.transform.LayerNorm.weight \n",
      "MODEL  replacing key lm_head.layer_norm.bias from  loaded statedic  with head.mlm.predictions.transform.LayerNorm.bias \n",
      "MODEL  replacing key lm_head.decoder.weight from  loaded statedic  with head.mlm.predictions.decoder.weight \n",
      "MODEL loading with  prefix , metadata {'version': 1} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.embeddings. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.embeddings.word_embeddings. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.embeddings.position_embeddings. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.embeddings.token_type_embeddings. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.embeddings.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.embeddings.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.0.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.1.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.2.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.3.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL loading with encoder.encoder.layer.4.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.4.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.5.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.6.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.7.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.8.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL loading with encoder.encoder.layer.9.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.9.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.10.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention.self. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention.self.query. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention.self.key. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention.self.value. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention.self.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.attention.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.intermediate. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.intermediate.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.output. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.output.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.output.LayerNorm. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.encoder.layer.11.output.dropout. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.pooler. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.pooler.dense. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with encoder.pooler.activation. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "MODEL loading with head. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "WARNING head. missing keys in state_dict []\n",
      "MODEL loading with head.pos. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "WARNING head.pos. missing keys in state_dict []\n",
      "MODEL loading with head.pos.classifier. prefix , metadata {} missing keys [] unexpected keys [] \n",
      "WARNING head.pos.classifier. missing keys in state_dict []\n",
      "MODEL loading with head.pos.dropout. prefix , metadata {} missing keys ['head.pos.classifier.weight', 'head.pos.classifier.bias'] unexpected keys [] \n"
     ]
    }
   ],
   "source": [
    "bert_model = \"camembert-cased-1\"\n",
    "tasks = [\"pos\"]\n",
    "vocab_size = \"10\"\n",
    "\n",
    "pretrained_model_dir = BERT_MODEL_DIC[bert_model][\"model\"]\n",
    "voc_tokenizer = BERT_MODEL_DIC[bert_model][\"vocab\"]\n",
    "encoder = BERT_MODEL_DIC[bert_model][\"encoder\"]#, \"BertModel\")\n",
    "tokenizer = eval(BERT_MODEL_DIC[bert_model][\"tokenizer\"])\n",
    "tokenizer = tokenizer.from_pretrained(voc_tokenizer, do_lower_case=False)\n",
    "mask_id = tokenizer.convert_tokens_to_ids(tokenizer.mask_token) #convert_tokens_to_ids([MASK_BERT])[0]\n",
    "num_labels_per_task, task_to_label_dictionary = get_vocab_size_and_dictionary_per_task([\"mlm\"],\n",
    "                                                                                       vocab_bert_wordpieces_len=vocab_size,\n",
    "                                                                                       pos_dictionary=None,#{\"pos\":vocab_size},\n",
    "                                                                                       type_dictionary=None,\n",
    "                                                                                       task_parameters=TASKS_PARAMETER)\n",
    "    \n",
    "\n",
    "model = BertMultiTask.from_pretrained(pretrained_model_dir,\n",
    "                                      tasks=tasks,\n",
    "                                      mask_id=mask_id,\n",
    "                                      #config_to_update=config_to_update,\n",
    "                                      num_labels_per_task={\"pos-pos\":10},\n",
    "                                      mapping_keys_state_dic=DIR_2_STAT_MAPPING[pretrained_model_dir],\n",
    "                                      encoder=eval(encoder) if encoder is not None else BertModel,\n",
    "                                      dropout_classifier=0.1,\n",
    "                                      hidden_dropout_prob=0.1,\n",
    "                                      random_init=False,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentence_conllu(directory, n_sent_max,get_lemma=False):\n",
    "    fr_sent_ls = []\n",
    "    arabizi_ls = []\n",
    "    new_sent = 0\n",
    "    #n_sent_max = 50n_sent_max\n",
    "    arabizi = \"\"\n",
    "\n",
    "\n",
    "    with open(directory, \"r\") as f:\n",
    "        print(\"READING \", directory)\n",
    "        for line in f :\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"#\"):\n",
    "                if len(arabizi)>6:\n",
    "                    new_sent+=1\n",
    "                    if get_lemma:\n",
    "                        fr_sent+=\" [SEP]\"\n",
    "                        fr_sent_ls.append(fr_sent)\n",
    "                    arabizi+= \" [SEP]\"\n",
    "                    arabizi_ls.append(arabizi)\n",
    "                if get_lemma:\n",
    "                    fr_sent = \"[CLS]\"\n",
    "                arabizi = \"[CLS]\"\n",
    "            elif not line.startswith(\"#\") and len(line)>0:\n",
    "                line = line.split(\"\\t\")\n",
    "                if \"-\" not in line[0]:\n",
    "                    if get_lemma:\n",
    "                        fr_sent+= \" \"+line[2]\n",
    "                    arabizi+= \" \"+line[1]\n",
    "            if new_sent==n_sent_max:\n",
    "                break\n",
    "    return arabizi_ls, fr_sent_ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING  /Users/bemuller/Documents/Work/INRIA/dev/parsing/data/Universal-Dependencies-2.4/es_ancora-ud-dev.conllu\n",
      "READING  /Users/bemuller/Documents/Work/INRIA/dev/parsing/data/Universal-Dependencies-2.4/fr_spoken-ud-dev.conllu\n",
      "READING  /Users/bemuller/Documents/Work/INRIA/dev/parsing/data/Universal-Dependencies-2.4/en_ewt-ud-dev.conllu\n"
     ]
    }
   ],
   "source": [
    "n_sent_max = 25\n",
    "es, _ = get_sentence_conllu(get_dir_data(\"dev\", \"es_ancora\"), n_sent_max=n_sent_max,get_lemma=False)\n",
    "#ar_padt, _ = get_sentence_conllu(get_dir_data(\"dev\", \"ar_padt\"), n_sent_max=n_sent_max,get_lemma=False)\n",
    "fr_sequoia_ls, _ = get_sentence_conllu(get_dir_data(\"dev\", \"fr_spoken\"), n_sent_max=n_sent_max,get_lemma=False)\n",
    "en_ewt_ls, _ = get_sentence_conllu(get_dir_data(\"dev\", \"en_ewt\"), n_sent_max=n_sent_max,get_lemma=False)\n",
    "#arabizi_ls, fr_sent_ls = get_sentence_conllu(ARABIZI_DEV_POS, n_sent_max=n_sent_max,get_lemma=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=1\n",
    "use_gpu =False\n",
    "#input_string = [\"[CLS] okay my friend is awesome [SEP]\"]\n",
    "\n",
    "#input_tokens_tensor_fr, input_segments_tensors, inp_bpe_tokenized, \\\n",
    "#input_alignement_with_raw, input_mask_fr = get_indexes(fr_sent_ls, tokenizer, verbose, use_gpu)\n",
    "\n",
    "input_tokens_tensor_arabizi, input_segments_tensors, inp_bpe_tokenized, \\\n",
    "input_alignement_with_raw, input_mask_ar = get_indexes(arabizi_ls, tokenizer, verbose, use_gpu)\n",
    "\n",
    "input_tokens_tensor_fr_sequoia, input_segments_tensors, inp_bpe_tokenized, \\\n",
    "input_alignement_with_raw, input_mask_fr_sequoia = get_indexes(fr_sequoia_ls, tokenizer, verbose, use_gpu)\n",
    "\n",
    "input_tokens_tensor_en, input_segments_tensors, inp_bpe_tokenized, \\\n",
    "input_alignement_with_raw, input_mask_en  = get_indexes(en_ewt_ls, tokenizer, verbose, use_gpu)\n",
    "\n",
    "input_tokens_tensor_es, input_segments_tensors, inp_bpe_tokenized, \\\n",
    "input_alignement_with_raw, input_mask_es  = get_indexes(es, tokenizer, verbose, use_gpu)\n",
    "\n",
    "input_tokens_tensor_ar_padt, input_segments_tensors, inp_bpe_tokenized, \\\n",
    "input_alignement_with_raw, input_mask_ar_padt  = get_indexes(ar_padt, tokenizer, verbose, use_gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
